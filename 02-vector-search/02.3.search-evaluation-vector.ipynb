{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8565c5ed",
   "metadata": {},
   "source": [
    "# Search Evaluation 2 - Vector Retrieval\n",
    "\n",
    "Reference [LLM Zoomcamp 3.3.4 - Evaluating Vector Retrieval\n",
    "](https://www.youtube.com/watch?v=VRprIm9-VV8)\n",
    "\n",
    "# Step 0: Start up docker for ElasticSearch\n",
    "```\n",
    "docker run -it \\\n",
    "    --rm \\\n",
    "    --name elasticsearch \\\n",
    "    -m 2GB \\\n",
    "    -p 9200:9200 \\\n",
    "    -p 9300:9300 \\\n",
    "    -e \"discovery.type=single-node\" \\\n",
    "    -e \"xpack.security.enabled=false\" \\\n",
    "    docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n",
    "```\n",
    "\n",
    "# Step 1: Load Documents into ElasticSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b991541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘documents-with-ids.json’ already there; not retrieving.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371807f227f84119a00bda4242874591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/948 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c008593f20e148158a13d1a0bac085c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/948 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "np.float_ = np.float64\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "import numpy as np\n",
    "np.float_ = np.float64\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# download documents\n",
    "!wget -nc https://raw.githubusercontent.com/slavaheroes/llm-zoomcamp/refs/heads/homeworks/03-vector-search/eval/documents-with-ids.json\n",
    "\n",
    "# load documents.json and flatten them\n",
    "with open('documents-with-ids.json', 'rt') as f_in:\n",
    "    documents = json.load(f_in)\n",
    "\n",
    "\n",
    "es_client = Elasticsearch('http://localhost:9200')\n",
    "\n",
    "# create mapping\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": 1,\n",
    "        \"number_of_replicas\": 0\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"},\n",
    "            \"id\": {\"type\": \"keyword\"},\n",
    "            \"question_vector\": {\n",
    "                \"type\": \"dense_vector\", \n",
    "                \"dims\": 384, \n",
    "                \"index\": True, \n",
    "                \"similarity\": \"cosine\"\n",
    "                },\n",
    "            \"text_vector\": {\n",
    "                \"type\": \"dense_vector\", \n",
    "                \"dims\": 384, \n",
    "                \"index\": True, \n",
    "                \"similarity\": \"cosine\"\n",
    "                },\n",
    "            \"question_text_vector\": {\n",
    "                \"type\": \"dense_vector\", \n",
    "                \"dims\": 384, \n",
    "                \"index\": True, \n",
    "                \"similarity\": \"cosine\"\n",
    "                },\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# create index\n",
    "index_name = \"course-questions\"\n",
    "\n",
    "es_client.indices.delete(index=index_name,ignore_unavailable=True)\n",
    "es_client.indices.create(index=index_name,body=index_settings)\n",
    "\n",
    "\n",
    "# initialize embeddings model\n",
    "model_name = 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# create embeddings\n",
    "for doc in tqdm(documents):\n",
    "    try:\n",
    "        question = doc['question'] \n",
    "        text = doc['text'] \n",
    "        qt = question +' ' + text\n",
    "\n",
    "        doc['question_vector'] = model.encode(question)\n",
    "        doc['text_vector'] = model.encode(text)\n",
    "        doc['question_text_vector'] = model.encode(qt)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "# add documents to index\n",
    "for doc in tqdm(documents):\n",
    "    try: \n",
    "        es_client.index(index=index_name, document=doc)\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bad83cb",
   "metadata": {},
   "source": [
    "# Step 2: Evaluate \n",
    "#### Step 2.1: Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800164c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define evaluation functions\n",
    "\n",
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['document']\n",
    "        results = search_function(q)\n",
    "        relevance = [d['id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }\n",
    "\n",
    "# define search function to evaluate\n",
    "def elastic_search_knn(field, vector, course):\n",
    "    knn = {\n",
    "            \"field\": field,\n",
    "            \"query_vector\": vector,\n",
    "            \"k\": 5,\n",
    "            \"num_candidates\": 1000,\n",
    "            \"filter\": {\n",
    "                \"term\": {\n",
    "                    \"course\": course\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "    query = {\n",
    "        \"knn\": knn,\n",
    "        \"_source\": [\"text\", \"section\", \"question\", \"course\", \"id\"]\n",
    "    }\n",
    "\n",
    "    es_results = es_client.search(\n",
    "        index=index_name, \n",
    "        body=query, \n",
    "        )\n",
    "    \n",
    "    result_docs = []\n",
    "    for hit in es_results[\"hits\"][\"hits\"]:\n",
    "        result_docs.append(hit['_source'])\n",
    "\n",
    "    return result_docs\n",
    "\n",
    "def question_vector_knn(q):\n",
    "    question = q['question']\n",
    "    course = q['course']\n",
    "\n",
    "    v_q = model.encode(question)\n",
    "    \n",
    "    return elastic_search_knn('question_vector', v_q, course)\n",
    "\n",
    "def text_vector_knn(q):\n",
    "    question = q['question']\n",
    "    course = q['course']\n",
    "\n",
    "    v_q = model.encode(question)\n",
    "    \n",
    "    return elastic_search_knn('text_vector', v_q, course)\n",
    "\n",
    "def question_text_vector_knn(q):\n",
    "    question = q['question']\n",
    "    course = q['course']\n",
    "\n",
    "    v_q = model.encode(question)\n",
    "    \n",
    "    return elastic_search_knn('question_text_vector', v_q, course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29098664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example on how to use search function\n",
    "search_term = 'I just discovered the course. Can I still join?'\n",
    "search_term_vector = model.encode(search_term)\n",
    "course = 'data-engineering-zoomcamp'\n",
    "\n",
    "elastic_search_knn('question_vector', search_term_vector, course)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80788c1e",
   "metadata": {},
   "source": [
    "#### Step 2.2: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4562a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ground truth dataset\n",
    "fn = 'https://raw.githubusercontent.com/slavaheroes/llm-zoomcamp/refs/heads/homeworks/03-vector-search/eval/ground-truth-data.csv'\n",
    "ground_truth = pd.read_csv(fn).to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ff37f32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac703705a0f54aacb8c98d314d64fb80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.773071104387292, 'mrr': 0.6666810748505158}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ground_truth, question_vector_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71f3bf42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e014f7983e714ef8a03d6d6069b0e479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.8286146531229739, 'mrr': 0.7062315395144454}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ground_truth, text_vector_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26469ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b9be637390415993a57a5dbf173ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4627 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.9172249837907932, 'mrr': 0.824306606152295}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ground_truth, question_text_vector_knn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
