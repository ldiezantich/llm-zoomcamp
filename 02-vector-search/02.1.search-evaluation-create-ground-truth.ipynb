{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8565c5ed",
   "metadata": {},
   "source": [
    "# Search Evaluation - Generate ground truth dataset\n",
    "\n",
    " notes following the [3.3.2 video](https://www.youtube.com/watch?v=bpxi6fKcyLw)\n",
    "\n",
    "# Step 1: Generate IDs for each record\n",
    "Here, I am using the already generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b991541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from huggingface_hub import InferenceClient\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def generate_document_id(doc):\n",
    "    combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n",
    "    hash_object = hashlib.md5(combined.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    document_id = hash_hex[:8]\n",
    "    return document_id\n",
    "\n",
    "# download documents\n",
    "!wget -nc https://raw.githubusercontent.com/DataTalksClub/llm-zoomcamp/refs/heads/main/01-intro/documents.json\n",
    "\n",
    "# load documents.json and flatten them\n",
    "with open('documents.json', 'rt') as f_in:\n",
    "    docs_raw = json.load(f_in)\n",
    "\n",
    "documents = []\n",
    "for course_dict in docs_raw:\n",
    "    for doc in course_dict['documents']:\n",
    "        doc['course'] = course_dict['course']\n",
    "        documents.append(doc)\n",
    "\n",
    "# add ID to document\n",
    "for doc in documents:\n",
    "    doc['id'] = generate_document_id(doc)\n",
    "\n",
    "with open('documents-with-ids.json', 'wt') as f_out:\n",
    "    json.dump(documents, f_out, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417c58ca",
   "metadata": {},
   "source": [
    "# Step 2: Generate questions for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8814838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, provider =\"novita\", model=\"deepseek-ai/DeepSeek-V3-0324\"):\n",
    "    client = InferenceClient(\n",
    "        provider=provider,\n",
    "        api_key=os.getenv('HF_API_KEY'),\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def generate_questions(doc, llm_params=None):\n",
    "    if llm_params is None:\n",
    "        llm_params = {}\n",
    "    prompt = prompt_template.format(**doc)\n",
    "    return llm(prompt, **llm_params)\n",
    "\n",
    "\n",
    "# create prompte template\n",
    "prompt_template = \"\"\"\n",
    "You emulate a student who's taking our course.\n",
    "Formulate 5 questions this student might ask based on a FAQ record. The record\n",
    "should contain the answer to the questions, and the questions should be complete and not too short.\n",
    "If possible, use as fewer words as possible from the record. \n",
    "\n",
    "The record:\n",
    "\n",
    "section: {section}\n",
    "question: {question}\n",
    "answer: {text}\n",
    "\n",
    "Provide the output in parsable JSON without using code blocks:\n",
    "\n",
    "[\"question1\", \"question2\", ..., \"question5\"]\n",
    "\"\"\".strip()\n",
    "\n",
    "# generate extra questions for each record\n",
    "results = {}\n",
    "for doc in tqdm(documents[:5]):\n",
    "    doc_id = doc['id']\n",
    "    if doc_id in results:\n",
    "        continue\n",
    "    questions = generate_questions(doc)\n",
    "    results[doc_id] = questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1047f42f",
   "metadata": {},
   "source": [
    "# Step 3: Parse results into ground truth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e667378",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_results = {}\n",
    "\n",
    "for doc_id, json_questions in results.items():\n",
    "    parsed_results[doc_id] = json.loads(json_questions)\n",
    "\n",
    "doc_index = {d['id']: d for d in documents}\n",
    "\n",
    "final_results = []\n",
    "for doc_id, questions in parsed_results.items():\n",
    "    course = doc_index[doc_id]['course']\n",
    "    for q in questions:\n",
    "        final_results.append((q, course, doc_id))\n",
    "\n",
    "df = pd.DataFrame(final_results, columns=['question', 'course', 'document'])\n",
    "df.to_csv('ground-truth-data.csv', index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
